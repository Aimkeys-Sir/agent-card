{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aimkeys-Sir/agent-card/blob/main/cardagent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Welcome to Poker EA agent**\n",
        "\n",
        "Switch the runtime type to GPU or TPU for faster runs."
      ],
      "metadata": {
        "id": "07V5uCB3TUOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b670BSwvnfRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CardAgent** class has an `__init__` method that initializes various parameters and creates the layers of the neural network using the nn.Linear module.\n",
        "\n",
        "The `network` method sets up the layers of the neural network and loads weights from a specified file path if `load_weights` is set to `True`.\n",
        "\n",
        "The `forward` method defines the forward pass of the neural network. It applies `ReLU` activation functions to the first three layers and a `softmax` activation function to the last layer. If a mask is provided, it applies element-wise multiplication between the `output` and the `mask`.\n",
        "\n",
        "The `remember` method is used to store experiences (`observation`, `move`, `reward`, `next_state`, `complete`) into the agent's memory deque.\n",
        "\n",
        "The `train_memory` method trains the neural network using a mini-batch of experiences from the memory. It calculates the `target` value based on the reward and the `maximum Q-value` of the next state (if the episode is not complete). Then, it performs a forward pass, calculates the loss using mean squared error (MSE) loss, and performs backpropagation to update the weights.\n",
        "\n",
        "The `replay_exp` method replays experiences from the memory and calls the `train_memory` method for each experience."
      ],
      "metadata": {
        "id": "cTqtt2cnRB97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import collections\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from typing import Type\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class CardAgent(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.first_layer = params[\"first_layer_size\"]\n",
        "        self.second_layer = params[\"second_layer_size\"]\n",
        "        self.third_layer = params[\"third_layer_size\"]\n",
        "        self.gamma = params[\"gamma\"]\n",
        "        self.learning_rate = params[\"learning_rate\"]\n",
        "        self.memory = collections.deque(maxlen= params[\"memory_size\"])\n",
        "        self.batch_size = params[\"batch_size\"]\n",
        "        self.weights_path = params[\"weights_path\"]\n",
        "        self.optimizer = None\n",
        "        self.load_weights = params[\"load_weights\"]\n",
        "        self.mask = None\n",
        "        self.cum_reward = 0\n",
        "        self.network()\n",
        "\n",
        "    def network(self):\n",
        "        self.requires_grad_ = False\n",
        "        self.fc1 = nn.Linear(57, self.first_layer)\n",
        "        self.fc2 = nn.Linear(self.first_layer, self.second_layer)\n",
        "        self.fc3 = nn.Linear(self.second_layer, self.third_layer)\n",
        "        self.fc4 = nn.Linear(self.third_layer, 60)\n",
        "\n",
        "        if self.load_weights:\n",
        "           self.model = self.load_state_dict(torch.load(self.weights_path))\n",
        "           print(\"weights loaded\")\n",
        "\n",
        "\n",
        "    def forward(self, observation):\n",
        "        x = F.relu(self.fc1(observation))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.softmax(self.fc4(x), dim=-1)\n",
        "\n",
        "        if self.mask != None:\n",
        "          print(f\"before mask: \\n{x}\")\n",
        "          return x * self.mask\n",
        "        return x\n",
        "\n",
        "    def remember(self, observation, move, reward, next_state, complete):\n",
        "        self.memory.append((observation, move, reward, next_state, complete))\n",
        "\n",
        "    def train_memory(self, observation, move, reward, next_state, complete):\n",
        "        self.train()\n",
        "        self.mask = None\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "        target = reward\n",
        "\n",
        "        state_tensor = torch.tensor(np.expand_dims(observation, 0), dtype=torch.float32, requires_grad=True)\n",
        "        next_state_tensor = torch.tensor(np.expand_dims(observation, 0), dtype=torch.float32, requires_grad = True)\n",
        "\n",
        "        if not complete:\n",
        "            target = reward + self.gamma * torch.max(self.forward(next_state_tensor))\n",
        "\n",
        "        output = self.forward(state_tensor)\n",
        "        try:\n",
        "          target_f = output.clone()\n",
        "          if target_f.shape == torch.Size([1,1,60]) :\n",
        "            output = output[0]\n",
        "            target_f = target_f[0]\n",
        "          target_f[0][np.argmax(move)] = target\n",
        "          target_f.detach()\n",
        "          self.optimizer.zero_grad()\n",
        "          loss = F.mse_loss(output, target_f)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "        except IndexError:\n",
        "          print(output.shape)\n",
        "          print(move)\n",
        "          raise ValueError(\"what is this?\")\n",
        "\n",
        "\n",
        "    def replay_exp(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            minibatch = random.sample(self.memory, self.batch_size)\n",
        "        else:\n",
        "            minibatch = self.memory\n",
        "\n",
        "        for observation, move, reward, next_state, complete in minibatch:\n",
        "            self.train_memory(observation, move, reward, next_state, complete)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T08:12:24.130610Z",
          "iopub.execute_input": "2023-06-26T08:12:24.131072Z",
          "iopub.status.idle": "2023-06-26T08:12:24.153216Z",
          "shell.execute_reply.started": "2023-06-26T08:12:24.131037Z",
          "shell.execute_reply": "2023-06-26T08:12:24.151962Z"
        },
        "trusted": true,
        "id": "kxRSMTDHzoOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `to_cs(n_l)` takes a list of numbers `n_l` and maps them to the corresponding card names in the` cs` list. It returns a list of card names based on the input numbers.\n",
        "\n",
        "params1 and params2 correspond to both player's agents."
      ],
      "metadata": {
        "id": "Imv13mMGL_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs =  [\n",
        "  \"ace_of_clubs\",\n",
        "  \"ace_of_spades\",\n",
        "  \"ace_of_hearts\",\n",
        "  \"ace_of_diamonds\",\n",
        "  \"2_of_clubs\",\n",
        "  \"2_of_spades\",\n",
        "  \"2_of_hearts\",\n",
        "  \"2_of_diamonds\",\n",
        "  \"3_of_clubs\",\n",
        "  \"3_of_spades\",\n",
        "  \"3_of_hearts\",\n",
        "  \"3_of_diamonds\",\n",
        "  \"4_of_clubs\",\n",
        "  \"4_of_spades\",\n",
        "  \"4_of_hearts\",\n",
        "  \"4_of_diamonds\",\n",
        "  \"5_of_clubs\",\n",
        "  \"5_of_spades\",\n",
        "  \"5_of_hearts\",\n",
        "  \"5_of_diamonds\",\n",
        "  \"6_of_clubs\",\n",
        "  \"6_of_spades\",\n",
        "  \"6_of_hearts\",\n",
        "  \"6_of_diamonds\",\n",
        "  \"7_of_clubs\",\n",
        "  \"7_of_spades\",\n",
        "  \"7_of_hearts\",\n",
        "  \"7_of_diamonds\",\n",
        "  \"8_of_clubs\",\n",
        "  \"8_of_spades\",\n",
        "  \"8_of_hearts\",\n",
        "  \"8_of_diamonds\",\n",
        "  \"9_of_clubs\",\n",
        "  \"9_of_spades\",\n",
        "  \"9_of_hearts\",\n",
        "  \"9_of_diamonds\",\n",
        "  \"10_of_clubs\",\n",
        "  \"10_of_spades\",\n",
        "  \"10_of_hearts\",\n",
        "  \"10_of_diamonds\",\n",
        "  \"jack_of_clubs\",\n",
        "  \"jack_of_spades\",\n",
        "  \"jack_of_hearts\",\n",
        "  \"jack_of_diamonds\",\n",
        "  \"queen_of_clubs\",\n",
        "  \"queen_of_spades\",\n",
        "  \"queen_of_hearts\",\n",
        "  \"queen_of_diamonds\",\n",
        "  \"king_of_clubs\",\n",
        "  \"king_of_spades\",\n",
        "  \"king_of_hearts\",\n",
        "  \"king_of_diamonds\",\n",
        "  \"black_joker\",\n",
        "  \"red_joker\",\n",
        "  \"choose clubs\",\n",
        "  \"choose spades\",\n",
        "  \"choose diamonds\",\n",
        "  \"choose hearts\",\n",
        "  \"complete build\",\n",
        "  \"pick card\"\n",
        "]\n",
        "\n",
        "params = dict()\n",
        "\n",
        "params[\"first_layer_size\"] = 256\n",
        "params[\"second_layer_size\"] = 128\n",
        "params[\"third_layer_size\"] = 84\n",
        "params[\"learning_rate\"] = 0.01\n",
        "params[\"memory_size\"] = 25000\n",
        "\n",
        "#Don't load weights on the first pass since there are none\n",
        "params[\"load_weights\"] = True\n",
        "params['train'] = True\n",
        "params[\"epsilon_decay_linear\"] = 0.01\n",
        "params[\"episodes\"] = 100\n",
        "params[\"batch_size\"] = 1000\n",
        "params[\"gamma\"] = 0.99\n",
        "\n",
        "\n",
        "params1 = params.copy()\n",
        "params2 = params.copy()\n",
        "\n",
        "#create these folders in your drive to save the weights\n",
        "params1[\"weights_path\"] = \"drive/MyDrive/pokerEa/weights/agent1/weights.h5\"\n",
        "params2[\"weights_path\"] = \"drive/MyDrive/pokerEa/weights/agent2/weights.h5\"\n",
        "\n",
        "questions = [28, 29, 30, 31, 51, 50, 49, 48, 47, 46, 45, 44]\n",
        "aces = [0, 1, 2, 3]\n",
        "punishers = [4, 5, 6, 7, 8, 9, 10, 11, 52, 53]\n",
        "all_cards_without_jokers = list(range(52))\n",
        "\n",
        "def to_cs(n_l):\n",
        "    h = []\n",
        "    for n in n_l:\n",
        "        h.append(cs[n])\n",
        "    return h\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T08:12:36.710107Z",
          "iopub.execute_input": "2023-06-26T08:12:36.710507Z",
          "iopub.status.idle": "2023-06-26T08:12:36.726269Z",
          "shell.execute_reply.started": "2023-06-26T08:12:36.710477Z",
          "shell.execute_reply": "2023-06-26T08:12:36.725152Z"
        },
        "trusted": true,
        "id": "RACYW4Y4zoOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Player** class represents a player in the card game.\n",
        "\n",
        "`__init__(self, game, index)`: Initializes the player object with attributes such as `hand`, `build`, `game`, `asking`, `index`, `reward`, and `agent`.\n",
        "\n",
        "`can_complete(self)`: Checks if the player can complete their build. Returns True if the build is not empty and the last card in the build is not in a specific list of cards (questions), aces, or punishers. Otherwise, returns False.\n",
        "\n",
        "`check_in_white(self)`: Returns a list of cards from the player's hand that are in the white list of cards (`game.white_list(build=self.build)`).\n",
        "\n",
        "`waste_card(self, card)`: Handles the logic when the player wastes a card. If the card is not in the player's hand, the `reward` is set to -1. Otherwise, if the card is in the white list, it is added to the player's build, removed from their hand, and the `reward` is set to 1. If the card is not in the white list, the `reward` is set to -1.\n",
        "\n",
        "`pick_cards(self)`: Handles the logic when the player picks cards. Calls the `pick` method of the game and starts a new turn. Clears the player's build. The `reward` is set based on the number of remaining cards in the player's hand.\n",
        "\n",
        "`complete_build(self)`: Handles the logic when the player completes their build. If the build is empty, the `reward` is set to -1. Otherwise, the player's build is wasted using the `waste` method of the game. If the player has no cards left and the build's last card is not in the list of questions, aces, or punishers, and there are no card-less players in the game, the player wins, the game is marked as complete, and the player's won count is incremented. Otherwise, if the player has no cards left and either the build's last card is in the list or there are card-less players in the game, the player is added to the list of card-less players. The player's build is cleared, and the `reward` is set to 3. If the player is not asking, a new turn is started.\n",
        "\n",
        "`choose_flower(self, flower)`: Handles the logic when the player chooses a flower. If the player is asking, the chosen flower is set as the game's action, a new turn is started, and the asking flag is set to False. Otherwise, the reward is set to -1.\n",
        "\n",
        "`do_move(self, move)`: Handles the logic for the player's move. The move parameter determines the action taken by the player. If the move is less than 54, the player wastes a card. If the move is between 54 and 57, the player chooses a flower. If the move is 58, the player completes their build. If the move is 59, the player picks cards. The reward is set based on the outcome of the move.\n",
        "\n",
        "`one_hot_encoded_hand(self)`: Returns a one-hot encoded representation of the player's hand using a tensor of size 54, where the indices corresponding to the player's cards are set to 1.0.\n",
        "\n",
        "`observation(self)`: Returns the observation of the player, which includes the one-hot encoded hand, the normalized value of the last card in the player's build or the top card of the game if the build is empty, the normalized value of the game's action"
      ],
      "metadata": {
        "id": "xD94D8nuNZo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Player():\n",
        "    def __init__(self, game, index) -> None:\n",
        "        self.hand = []\n",
        "        self.build = []\n",
        "        self.game = game\n",
        "        self.asking = False\n",
        "        self.index = index\n",
        "        self.reward = 0\n",
        "        self.won = 0\n",
        "        self.agent = None\n",
        "\n",
        "    def can_complete(self):\n",
        "        if len(self.build) == 0 or self.build[-1] in questions:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def check_in_white(self):\n",
        "         return [card for card in self.hand if card in self.game.white_list(build=self.build)]\n",
        "\n",
        "    def waste_card(self, card):\n",
        "        if card not in self.hand:\n",
        "          self.reward = -1\n",
        "          return\n",
        "\n",
        "        white = self.game.white_list(build=self.build)\n",
        "        if card in white:\n",
        "            self.build += [card]\n",
        "            # print(f\"build:{ self.build} card: {card}\")\n",
        "            self.hand = list(filter(lambda x: x != card, self.hand))\n",
        "            self.reward = 1\n",
        "        else:\n",
        "            # print(f\"build:{ self.build} card: {card}\")\n",
        "            self.reward = -1\n",
        "            # wrong move\n",
        "\n",
        "    def pick_cards(self):\n",
        "        self.game.pick(player=self)\n",
        "        self.game.new_turn()\n",
        "        self.build.clear()\n",
        "        self.reward = ((len(self.hand)-4) / 2) * - \\\n",
        "            1 if len(self.hand) > 6 else 0\n",
        "\n",
        "    def complete_build(self):\n",
        "        if len(self.build) == 0:\n",
        "            self.reward = -1\n",
        "            return\n",
        "\n",
        "        self.game.waste(self)\n",
        "\n",
        "        if len(self.hand) == 0 and self.build[-1] not in questions + aces + punishers and len(self.game.card_less) == 0:\n",
        "            self.reward = 20\n",
        "            self.game.complete = True\n",
        "            self.game.winner = self.index\n",
        "            self.won +=1\n",
        "            self.build = []\n",
        "            return\n",
        "        elif len(self.hand) == 0 and (self.build[-1] in questions+aces+punishers or len(self.game.card_less)>0):\n",
        "            self.game.card_less += [self.index]\n",
        "\n",
        "        self.build.clear()\n",
        "        self.reward = 1\n",
        "        if not self.asking:\n",
        "            self.game.new_turn()\n",
        "\n",
        "    def choose_flower(self, flower):\n",
        "        if self.asking:\n",
        "            self.game.action = flower\n",
        "            self.game.new_turn()\n",
        "            self.asking = False\n",
        "        else:\n",
        "            self.reward = -1\n",
        "\n",
        "    def do_move(self, move):\n",
        "        self.reward = 0\n",
        "        if move < 54:\n",
        "            self.waste_card(move)\n",
        "        elif move > 53 and move < 58:\n",
        "            self.choose_flower(move-54)\n",
        "        elif move == 58:\n",
        "            self.complete_build()\n",
        "        elif move == 59:\n",
        "            self.pick_cards()\n",
        "\n",
        "    def one_hot_encoded_hand(self):\n",
        "        try:\n",
        "            tensor = torch.zeros(54)\n",
        "            tensor[self.hand] = 1.0\n",
        "            return tensor\n",
        "        except IndexError:\n",
        "            print(self.hand, self.game.deck)\n",
        "            raise ValueError(\"happened again\")\n",
        "\n",
        "    def observation(self):\n",
        "        top = self.build[-1] if len(self.build)> 0 else self.game.top_card\n",
        "        top_normal = top / 53\n",
        "\n",
        "        actions = 0 if self.game.action == -1 else self.game.action + 1\n",
        "        actions_normal = actions / 8\n",
        "        cardless = 1 if len(self.game.card_less) > 0 else 0\n",
        "\n",
        "        return torch.cat([self.one_hot_encoded_hand(), torch.tensor([top_normal, actions_normal, cardless], requires_grad = False)], dim=0)\n",
        "\n",
        "    def mask(self):\n",
        "        white = self.game.white_list(build = self.build)\n",
        "        match = []\n",
        "        for card in self.hand:\n",
        "            if card in white:\n",
        "                match.append(card)\n",
        "\n",
        "        complete = 1 if self.can_complete() else 0\n",
        "        can_pick = 0 if (len(self.build)>0 and self.build[-1] not in questions) else 1\n",
        "\n",
        "        if self.asking:\n",
        "            return torch.cat([torch.zeros(54),torch.tensor([1,1,1,1], requires_grad=False), torch.zeros(2)])\n",
        "        else:\n",
        "            match_t = torch.zeros(58)\n",
        "            match_t[match] = 1.0\n",
        "\n",
        "            return torch.cat([match_t, torch.tensor([complete, can_pick], requires_grad= False)], dim=0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T08:12:41.659433Z",
          "iopub.execute_input": "2023-06-26T08:12:41.659819Z",
          "iopub.status.idle": "2023-06-26T08:12:41.688858Z",
          "shell.execute_reply.started": "2023-06-26T08:12:41.659790Z",
          "shell.execute_reply": "2023-06-26T08:12:41.687550Z"
        },
        "trusted": true,
        "id": "pxlQtbHQzoOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`__init__(self)`: Initializes the game object with attributes: `complete`, `wastes`, `deck`, `top_card`, `action`, `turn`, `card_less`, and `winner`.\n",
        "\n",
        "`new_turn(self)`: Advances the `turn` to the next player by incrementing the turn attribute modulo 2.\n",
        "\n",
        "`waste(self, player)`: Handles the logic when a player wastes their build. If the action is between 4 and 6 (punishers) and the top card of the player's build is less than 4, the player is required to have at least 2 aces in their build to proceed. If they don't, the top card is changed to the previous top card from the wastes, the turn is advanced to the next player, and the player's build is inserted back into the wastes before the previous top card. Otherwise, the player is set to be asking. In any other case, the player's build is added to the wastes. The top card is updated to the last card in the wastes.\n",
        "\n",
        "`pick(self, player)`: Handles the logic when a player picks cards. The pick_num is determined based on the current action. If the action is 4, the player picks 2 cards. If the action is 5, the player picks 3 cards. If the action is 6, the player picks 5 cards. If the deck has fewer cards than the required number, the deck is replenished with the cards from the wastes (except the last card), and the top card is updated. The player picks random cards from the deck and adds them to their hand. If the player was previously card-less, they are removed from the card_less list.\n",
        "\n",
        "`white_list(self, build=[])`: Returns the white list of cards based on the current action and the build. If there is an action, the white list is determined accordingly. If the action is between 0 and 3 (pattern), the white list includes cards that match the pattern and aces. If the action is 4, the white list includes cards 4, 5, 6, 7, and aces. If the action is 5, the white list includes cards 8, 9, 10, 11, and aces. If the action is 6, the white list includes cards 52, 53, and aces. If the action is 7 (jump), the white list includes cards with a value of 10. If there is no action and the build is empty, the white list is determined based on the top card. If the top card is 52, the white list includes cards with pattern 0 or 1, aces, and cards 52 and 53. If the top card is 53, the white list includes cards with pattern 2 or 3, aces, and cards 52 and 53. Otherwise, the white list includes cards with the same pattern or value as the top card, aces, and either card 52 or 53 based on the top card's pattern. If the build is not empty, the white list is determined based on the last card in the build. If the last card is a question, the white list includes cards with the same pattern or value as the last card, aces, and either card 52 or 53 based on the top card's pattern. If the last card is 52 or 53, the white list includes only cards 52 and 53. Otherwise, the white list includes cards with the same value as the last card."
      ],
      "metadata": {
        "id": "FyYSIqIxMksp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Game():\n",
        "    def __init__(self) -> None:\n",
        "        self.complete = True\n",
        "        self.wastes = []\n",
        "        self.deck = []\n",
        "        self.top_card = None\n",
        "        self.action = -1     # -1 - no action, 0-3 pattern(clubs, spades, diamonds, hearts), 4-6 punishers(pick 2,3,5), 7- jump\n",
        "        self.turn = 0\n",
        "        self.card_less = []\n",
        "        self.winner = None\n",
        "\n",
        "    def new_turn(self):\n",
        "        self.turn = (self.turn+1) % 2\n",
        "\n",
        "    def waste(self, player):\n",
        "        self.action = -1\n",
        "        top = player.build[-1]\n",
        "        if self.action > 3 and top < 4:\n",
        "            aces_in_build = list(filter(lambda x: x < 4, player.build))\n",
        "\n",
        "            if len(aces_in_build) < 2:\n",
        "                top = self.wastes[-1]\n",
        "                self.turn = (self.turn+1) % 2\n",
        "                self.wastes = self.wastes[0:-1] + \\\n",
        "                    player.build + self.wastes[-1:]\n",
        "            else:\n",
        "                player.asking = True\n",
        "        elif top < 4:\n",
        "            player.asking = True\n",
        "            self.wastes += player.build\n",
        "        else:\n",
        "            self.wastes += player.build\n",
        "\n",
        "        self.top_card = self.wastes[-1]\n",
        "\n",
        "        if self.top_card in punishers:\n",
        "            if math.floor(self.top_card/4) == 2:\n",
        "                self.action = 4\n",
        "            elif math.floor(self.top_card/4) == 3:\n",
        "                self.action = 5\n",
        "            elif self.top_card in [52, 53]:\n",
        "                self.action = 6\n",
        "\n",
        "    def pick(self, player):\n",
        "        pick_num = 1\n",
        "        if self.action == 4:\n",
        "            pick_num = 2\n",
        "            self.action = -1\n",
        "        elif self.action == 5:\n",
        "            pick_num = 3\n",
        "            self.action = -1\n",
        "        elif self.action == 6:\n",
        "            pick_num = 5\n",
        "            self.action = -1\n",
        "\n",
        "        #if there's not enough cards on deck, reshuffle\n",
        "        if len(self.deck) < pick_num:\n",
        "            self.deck += self.wastes[0:-1]\n",
        "            self.wastes = self.wastes[-1:]\n",
        "            self.top_card = self.wastes[-1]\n",
        "\n",
        "        picked_cards = random.sample(self.deck, pick_num)\n",
        "        self.deck = list(filter(lambda x: x not in picked_cards, self.deck))\n",
        "\n",
        "        player.hand += picked_cards\n",
        "\n",
        "        #if the player was cardless, remove them from card_less list\n",
        "        if player.index in self.card_less:\n",
        "            self.card_less = list(filter(lambda x: x != player.index, self.card_less))\n",
        "\n",
        "    def white_list(self, build=[]):\n",
        "        if self.action != -1:\n",
        "            if self.action < 4:\n",
        "                white = list(filter(lambda x: x %\n",
        "                             4 == self.action, all_cards_without_jokers))\n",
        "                if self.action < 2:\n",
        "                    white += [52]\n",
        "                else:\n",
        "                    white += [53]\n",
        "                return white + aces\n",
        "            elif self.action == 4:\n",
        "                return [4, 5, 6, 7] + aces\n",
        "            elif self.action == 5:\n",
        "                return [8, 9, 10, 11] + aces\n",
        "            elif self.action == 6:\n",
        "                return [52, 53] + aces\n",
        "            elif self.action == 7:\n",
        "                return list(filter(lambda x: math.floor(x/4) == 10, all_cards_without_jokers))\n",
        "\n",
        "        if len(build) == 0:\n",
        "            if self.top_card == 52:\n",
        "                white = list(filter(lambda x: x %\n",
        "                             4 < 2, all_cards_without_jokers))\n",
        "                white += aces + [52, 53]\n",
        "            elif self.top_card == 53:\n",
        "                white = list(filter(lambda x: x %\n",
        "                             4 > 1, all_cards_without_jokers))\n",
        "                white += aces + [52, 53]\n",
        "            else:\n",
        "                white = list(filter(lambda x: x % 4 == self.top_card % 4 or math.floor(\n",
        "                    x/4) == math.floor(self.top_card/4), all_cards_without_jokers))\n",
        "                if self.top_card % 4 < 2:\n",
        "                    white += [52]\n",
        "                else:\n",
        "                    white += [53]\n",
        "            return white\n",
        "        else:\n",
        "            last = build[-1]\n",
        "            if last in questions:\n",
        "                white = list(filter(lambda x: x % 4 == last % 4 or math.floor(\n",
        "                    x/4) == math.floor(last/4), all_cards_without_jokers))\n",
        "                if self.top_card % 4 < 2:\n",
        "                    white += [52]\n",
        "                else:\n",
        "                    white += [53]\n",
        "                return white + aces\n",
        "            elif last == 52 or last == 53:\n",
        "                return [52, 53]\n",
        "            else:\n",
        "                white = list(filter(lambda x: math.floor(\n",
        "                    x/4) == math.floor(last/4), all_cards_without_jokers))\n",
        "                return white\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T08:12:46.631184Z",
          "iopub.execute_input": "2023-06-26T08:12:46.631644Z",
          "iopub.status.idle": "2023-06-26T08:12:46.667564Z",
          "shell.execute_reply.started": "2023-06-26T08:12:46.631606Z",
          "shell.execute_reply": "2023-06-26T08:12:46.665985Z"
        },
        "trusted": true,
        "id": "fGnth5bazoOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Game Initialization:**\n",
        "\n",
        "* The `initialize_game` function initializes the\n",
        "game by setting up the deck, distributing cards to players, selecting a top card, and initializing other game-related variables.\n",
        "\n",
        "**Play Function:**\n",
        "\n",
        "* The `play` function defines the main logic for a player's turn.\n",
        "* It first obtains the current state of the player using `player.observation()`.\n",
        "* Then, it uses the agent's epsilon-greedy strategy to choose an action. If a random number is less than the agent's epsilon value, a random action is chosen. Otherwise, the agent's neural network is used to predict the action.\n",
        "* The chosen action is executed by the player using `player.do_move(move)`.\n",
        "* The resulting reward is printed and stored in the player's reward attribute.\n",
        "* The next state is obtained, and the move is converted into a one-hot encoding representation.\n",
        "* The agent's remember and train_memory methods are called to store the experience in the agent's memory and train the agent's neural network.\n",
        "\n",
        "**Run Function:**\n",
        "\n",
        "* The `run` function is the main driver of the game and training process.\n",
        "* It initializes two instances of the CardAgent class, one for each player.\n",
        "* It sets up the optimizers for both agents.\n",
        "* It sets the player's agent attribute to the corresponding agent.\n",
        "* The main loop iterates until the desired number of games (`params['episodes']`) is reached.\n",
        "* Within the loop, if the game is complete, the game is initialized using `initialize_game`.\n",
        "* Then, the players take turns playing their moves using the play function.\n",
        "* The game progress is printed, and the step count is incremented. If the step count exceeds a threshold of 1000, the game is forced to complete.\n",
        "* After the game is complete, the rewards and experiences are updated, and the agents' memories are replayed (`replay`) to train the neural networks.\n",
        "*The loop continues until the desired number of games is completed."
      ],
      "metadata": {
        "id": "XZvqzukhPnvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_game(game, players):\n",
        "    game.deck = list(range(54))\n",
        "\n",
        "    for player in players:\n",
        "        player.hand = random.sample(game.deck, 4)\n",
        "\n",
        "        game.deck = list(filter(lambda x: x not in player.hand, game.deck))\n",
        "\n",
        "    poss_top = list(\n",
        "        filter(lambda x: x not in questions+aces+punishers, game.deck))\n",
        "    game.top_card = random.choice(poss_top)\n",
        "    game.wastes.append(game.top_card)\n",
        "\n",
        "    game.deck = list(filter(lambda x: x != game.top_card, game.deck))\n",
        "    game.winner = None\n",
        "    game.complete = False\n",
        "\n",
        "\n",
        "game = Game()\n",
        "player1 = Player(game=game, index=0)\n",
        "player2 = Player(game=game, index=1)\n",
        "\n",
        "def play(player, agent):\n",
        "    state = player.observation()\n",
        "    print(f\"\\nplayer {player.index+1}\")\n",
        "    if random.uniform(0,1) < agent.epsilon:\n",
        "        prediction = torch.rand(60)\n",
        "        prediction = prediction * player.mask()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.expand_dims(state, 0), dtype=torch.float32, requires_grad=False)\n",
        "            agent.mask = player.mask()\n",
        "            prediction = agent(state)\n",
        "            print(f\"agentPred: {prediction}\")\n",
        "\n",
        "    move = np.argmax(prediction).cpu().detach().numpy().item()\n",
        "\n",
        "    print(f\"move: {move}:{to_cs([move])}\")\n",
        "\n",
        "    player.do_move(move)\n",
        "    print(f\"reward: {player.reward}\")\n",
        "\n",
        "    next_state = player.observation()\n",
        "    m = np.eye(60)[np.argmax(prediction).numpy()]\n",
        "\n",
        "    agent.cum_reward += player.reward\n",
        "\n",
        "    agent.remember(observation=state, move=m, reward=player.reward, next_state=next_state, complete=player.game.complete)\n",
        "    agent.train_memory(observation=state, move=m, reward=player.reward, next_state=next_state, complete=player.game.complete)\n",
        "\n",
        "\n",
        "def std_mean_dev(array):\n",
        "  return statistics.mean(array) , statistics.stdev(array)\n",
        "\n",
        "\n",
        "def plot_seaborn(array_counter, array_score, y_name, train=True):\n",
        "    sns.set(color_codes=True, font_scale=1.5)\n",
        "    sns.set_style(\"white\")\n",
        "    plt.figure(figsize=(13,8))\n",
        "    fit_reg = False if train== False else True\n",
        "    ax = sns.regplot(\n",
        "        x=np.array(array_counter),\n",
        "        y=np.array(array_score),\n",
        "        #color=\"#36688D\",\n",
        "        x_jitter=.1,\n",
        "        scatter_kws={\"color\": \"#36688D\"},\n",
        "        label='Data',\n",
        "        fit_reg = fit_reg,\n",
        "        line_kws={\"color\": \"#F49F05\"}\n",
        "    )\n",
        "    # Plot the average line\n",
        "    y_mean = [np.mean(array_score)]*len(array_counter)\n",
        "    ax.plot(array_counter,y_mean, label='Mean', linestyle='--')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set(xlabel='# games', ylabel=y_name)\n",
        "    plt.show()\n",
        "    plt.savefig(f\"drive/MyDrive/pokerEa/plots/{y_name}.png\")\n",
        "\n",
        "def run():\n",
        "    agent1 = CardAgent(params=params1)\n",
        "    agent1.optimizer = optim.Adam(\n",
        "        agent1.parameters(), weight_decay=0, lr=params1['learning_rate'])\n",
        "    agent2 = CardAgent(params=params2)\n",
        "    agent2.optimizer = optim.Adam(\n",
        "        agent2.parameters(), weight_decay=0, lr=params2['learning_rate'])\n",
        "    games_count = 0\n",
        "    steps = 0\n",
        "    counter_plot = []\n",
        "    score1_plot = []\n",
        "    wins1_plot = []\n",
        "    score2_plot = []\n",
        "    wins2_plot = []\n",
        "\n",
        "    player1.agent = agent1\n",
        "    player2.agent = agent2\n",
        "\n",
        "    def replay(agent):\n",
        "        agent.replay_exp()\n",
        "        model_weights = agent.state_dict()\n",
        "        torch.save(model_weights, agent.weights_path)\n",
        "\n",
        "    while games_count < params['episodes']:\n",
        "        if game.complete:\n",
        "            steps = 0\n",
        "            initialize_game(game=game, players=[player1, player2])\n",
        "            print(\"\\nhands\")\n",
        "\n",
        "            print(to_cs(player1.hand))\n",
        "            print(to_cs(player2.hand))\n",
        "\n",
        "            print(\"\\n top card\")\n",
        "            print(cs[game.top_card])\n",
        "\n",
        "        while not game.complete:\n",
        "            if game.turn == 0:\n",
        "                if not params1['train']:\n",
        "                    agent1.epsilon = 0.01\n",
        "                else:\n",
        "                    agent1.epsilon = 1 - (games_count * params1[\"epsilon_decay_linear\"])\n",
        "\n",
        "                play(player=player1, agent=agent1)\n",
        "            elif game.turn == 1:\n",
        "                if not params2['train']:\n",
        "                    agent2.epsilon = 0.01\n",
        "                else:\n",
        "                    agent2.epsilon = 1 - \\\n",
        "                        (games_count * params1[\"epsilon_decay_linear\"])\n",
        "                play(player=player2, agent=agent2)\n",
        "\n",
        "\n",
        "            print(f\"game: {games_count}.  step: {steps} turn: {game.turn} score: {player1.won} - {player2.won}\")\n",
        "            steps += 1\n",
        "            if steps>1000:\n",
        "                game.complete = True\n",
        "            if game.complete:\n",
        "              if game.winner:\n",
        "                for p in [player1, player2] :\n",
        "                  if p.index != game.winner:\n",
        "                    p.reward = -20\n",
        "                    p.agent.cum_reward = -20\n",
        "                    state = p.observation()\n",
        "                    m = np.eye(60)\n",
        "                    p.agent.remember(observation=state, move=m, reward=p.reward, next_state=state, complete=p.game.complete)\n",
        "                    p.agent.train_memory(observation=state, move=m, reward=p.reward, next_state=state, complete=p.game.complete)\n",
        "\n",
        "              games_count += 1\n",
        "\n",
        "              score1_plot.append(agent1.cum_reward/steps)\n",
        "              score2_plot.append(agent2.cum_reward/steps)\n",
        "\n",
        "              wins1_plot.append(player1.won)\n",
        "              wins2_plot.append(player2.won)\n",
        "\n",
        "              counter_plot.append(games_count)\n",
        "\n",
        "              replay(agent=agent1)\n",
        "              replay(agent=agent2)\n",
        "\n",
        "    mean1, stdev1 = std_mean_dev(score1_plot)\n",
        "    print(f\"\\n\\nplayer 1: \\n mean reward: {mean1} \\n stdev: {stdev1}\")\n",
        "    print(\"\\nmean score vs games\")\n",
        "    plot_seaborn(counter_plot, score1_plot, \"01_scores_plot\")\n",
        "    print(\"\\nwins against games\")\n",
        "    plot_seaborn(counter_plot, wins1_plot, \"01_wins_plot\")\n",
        "\n",
        "    mean2, stdev2 = std_mean_dev(score2_plot)\n",
        "    print(f\"\\n\\nplayer 2: \\n mean reward: {mean2} \\n stdev: {stdev2}\")\n",
        "    print(\"\\nmean score vs games\")\n",
        "    plot_seaborn(counter_plot, score2_plot, \"02_scores_plot\")\n",
        "    print(\"\\nwins against games\")\n",
        "    plot_seaborn(counter_plot, wins2_plot, \"02_wins_plot\")\n",
        "\n",
        "run()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-26T08:12:52.530137Z",
          "iopub.execute_input": "2023-06-26T08:12:52.530579Z",
          "iopub.status.idle": "2023-06-26T08:12:52.772254Z",
          "shell.execute_reply.started": "2023-06-26T08:12:52.530547Z",
          "shell.execute_reply": "2023-06-26T08:12:52.770400Z"
        },
        "trusted": true,
        "id": "MIvdIqAGzoOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}